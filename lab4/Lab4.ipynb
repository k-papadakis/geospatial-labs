{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rslab-ntua/MSc_GBDA/blob/master/2022/Lab4.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pytorch-lightning av\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download lightweight version of UCF101 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://git.io/JGc31 -O ucf101_top5.tar.gz\n",
    "!mkdir data\n",
    "!tar xf ucf101_top5.tar.gz --directory data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a data pipeline for NN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Callable, List\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.io import read_video\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.transforms import ConvertImageDtype, Resize, Normalize\n",
    "import pickle\n",
    "\n",
    "# Define data root directory\n",
    "DATA_ROOT = \"data/\"\n",
    "\n",
    "# Define a Video-transform type\n",
    "VTransform = Callable[[torch.Tensor], torch.Tensor]\n",
    "\n",
    "class UCF101(Dataset):\n",
    "    def __init__(self, data_root, mode=\"train\", video_transforms: List[VTransform]=[], use_precomputed=True):\n",
    "        '''\n",
    "        Return a UCF101 Dataset instance\n",
    "        '''\n",
    "        super().__init__()\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        \n",
    "        self.root = data_root\n",
    "        self.mode = mode\n",
    "        self.v_transforms = video_transforms\n",
    "        \n",
    "        # Build database of samples\n",
    "        self._build_db()\n",
    "        \n",
    "        # Features precomute functionality\n",
    "        self.pre = use_precomputed\n",
    "        self.pre_root = os.path.join(self.root, \"precomp\")\n",
    "        if self.pre and not os.path.exists(self.pre_root):\n",
    "            os.makedirs(self.pre_root)\n",
    "        \n",
    "    def _build_db(self):\n",
    "        '''\n",
    "        Parse train/test csv containing paths to videos and corresponding labels.\n",
    "        Also, assign a unique index to each category\n",
    "        '''\n",
    "        csv_file = os.path.join(self.root, self.mode + \".csv\")\n",
    "        self.db: np.ndarray = pd.read_csv(csv_file, header=0).values\n",
    "        \n",
    "        unique_categories = np.sort(np.unique(self.db.T[1]))\n",
    "        self.categories = {c_name: c_idx for c_idx, c_name in enumerate(unique_categories)}\n",
    "        \n",
    "    def compute_sample(self, video_name, category):\n",
    "        '''\n",
    "        For a specific video, read data into memory, permute data to NumFrames x Channels x Height x Width format.\n",
    "        Also, transform data according to list of transforms\n",
    "        '''\n",
    "        \n",
    "        # Load video\n",
    "        V, *_ = read_video(os.path.join(self.root, self.mode, video_name))\n",
    "        # Permute data to NxCxHxW from NxHxWxC\n",
    "        V = V.permute(0,3,1,2)\n",
    "        \n",
    "        for T in self.v_transforms:\n",
    "            V = T(V)\n",
    "        \n",
    "        return V, self.categories[category]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Retrieve a specific sample from the dataset\n",
    "        '''\n",
    "        video_name, category = self.db[index]\n",
    "        \n",
    "        hval = \"_\".join([\n",
    "            self.mode,\n",
    "            video_name\n",
    "        ])\n",
    "        \n",
    "        if os.path.exists(os.path.join(self.pre_root, f\"{hval}.tmp\")):\n",
    "            with open(os.path.join(self.pre_root, f\"{hval}.tmp\"), \"rb\") as f:\n",
    "                sample =  pickle.load(f)\n",
    "        else:\n",
    "            sample = self.compute_sample(video_name, category)\n",
    "            # Save tmp\n",
    "            with open(os.path.join(self.pre_root, f\"{hval}.tmp\"), \"wb\") as f:\n",
    "                pickle.dump(sample, f)\n",
    "            \n",
    "        return sample\n",
    "            \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of samples in the dataset\n",
    "        '''\n",
    "        return self.db.shape[0]\n",
    "\n",
    "    \n",
    "def compute_features() -> VTransform:\n",
    "    '''\n",
    "    Returns a VTransform object that uses a pretrained CNN to extract features\n",
    "    '''\n",
    "    # Instantiate a CNN for feature extraction\n",
    "    encoder = resnet18(pretrained=True, progress=False)\n",
    "    # model = nn.Sequential(*list(encoder.children())[:-1], nn.Flatten())\n",
    "    model = create_feature_extractor(encoder, [\"avgpool\"])\n",
    "    model.eval()\n",
    "    \n",
    "    def apply(v: torch.Tensor) -> torch.Tensor:    \n",
    "        # return model(v)\n",
    "        with torch.no_grad():\n",
    "            feats = torch.flatten(model(v)[\"avgpool\"], 1)\n",
    "        return feats\n",
    "    \n",
    "    return apply\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataloaders and batching strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define train/val datasets\n",
    "train_dset = UCF101(DATA_ROOT, \"train\", video_transforms=[\n",
    "        ConvertImageDtype(torch.float32),\n",
    "        Resize((224, 224)),\n",
    "        Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]),\n",
    "        compute_features()\n",
    "    ])\n",
    "\n",
    "val_dset = UCF101(DATA_ROOT, \"test\", video_transforms=[\n",
    "        ConvertImageDtype(torch.float32),\n",
    "        Resize((224, 224)),\n",
    "        Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]),\n",
    "        compute_features()\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pad_sequences_collate_fn(samples: List[tuple]) -> tuple:\n",
    "    '''\n",
    "    Zero-pad (in front) each sample to enable batching. The longest sequence defines the sequence length for the batch\n",
    "    '''\n",
    "    \n",
    "    labels = torch.stack([torch.tensor(v[1]) for v in samples])\n",
    "    data = pad_sequence([v[0] for v in samples])\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "# Define train/val dataloaders with custom collate_fn for batching\n",
    "train_dloader = DataLoader(train_dset, batch_size=16, collate_fn=pad_sequences_collate_fn, shuffle=True, num_workers=4)\n",
    "val_dloader = DataLoader(val_dset, batch_size=16, collate_fn=pad_sequences_collate_fn, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a CNN-RNN Seq2Vec architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy, ConfusionMatrix\n",
    "\n",
    "class Seq2Vec(pl.LightningModule):\n",
    "    def __init__(self, features_in, num_classes, learning_rate=1e-3):\n",
    "        '''\n",
    "        Returns a Seq2Vec RNN model\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn_encoder = nn.GRU(\n",
    "            input_size=features_in,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=False,\n",
    "            dropout=0.3)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "\n",
    "        self.val_accuracy = Accuracy()        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward-pass\n",
    "        '''\n",
    "        rnn_out, h_n = self.rnn_encoder(x)\n",
    "        #  rnn_out: L, B, 32\n",
    "        return self.classifier(rnn_out[-1])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''\n",
    "        Training logic\n",
    "        '''\n",
    "        X, y = batch\n",
    "\n",
    "        logits = self(X)\n",
    "\n",
    "        loss = F.nll_loss(torch.log_softmax(logits, dim=-1), y)\n",
    "        self.log(\"loss/train\", loss, on_epoch=True, on_step=False, batch_size=X.size()[1])\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''\n",
    "        Validation logic\n",
    "        '''\n",
    "        X, y = batch\n",
    "\n",
    "        logits = self(X)\n",
    "\n",
    "        loss = F.nll_loss(torch.log_softmax(logits, dim=-1), y)\n",
    "        self.log(\"loss/val\", loss, on_epoch=True, on_step=False, batch_size=X.size()[1])\n",
    "\n",
    "        self.val_accuracy(logits, y)\n",
    "        self.log(\"accuracy/val\", self.val_accuracy, on_epoch=True, on_step=False, batch_size=X.size()[1])\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        Setup Adam optimizer\n",
    "        '''\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"accuracy/val\", mode=\"max\", patience=50),\n",
    "    ModelCheckpoint(monitor=\"accuracy/val\", mode=\"max\", save_last=True)\n",
    "]\n",
    "\n",
    "model = Seq2Vec(512, len(train_dset.categories), learning_rate=1e-4)\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\", \n",
    "    devices=1,\n",
    "    min_epochs=300,\n",
    "    max_epochs=1000,\n",
    "    callbacks=callbacks,\n",
    "    default_root_dir=\"seq2vec_gru\"\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_dloader, val_dataloaders=val_dloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir seq2vec_gru/lightning_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24bc0a32eaafbb00474073751844c35a309111b6e19aeb4c9a103d7e3fd87a74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
